**Expectations:** Should produce Precision = 1 and Recall = 1 on upper_bound evaluation

### Number of senses in TWSI clusters:
2333

### Hands-on analysis of twsi clusters

...

### Pooling of sense vectors (mean method) for wiki word vectors

**Expectations:** no parsing errors, hopefully no ommited small clusters

**Call:**

```
time ./pooling.py context-eval/data/Inventory-TWSI-2.csv 2333 model/wiki-sz300-w3-cb1-it3-min20.w2v model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.mean -inventory intermediate/twsi-clusters_inventory.csv --no_header
```

**Output:**

```
see intermediate/twsi_poolmean.log
```

**Observations:**
36 small clusters. For the sake of this experiment, don't filter out small clusters and repeat pooling

### Repeat pooling of sense vectors (any cluster size, mean method)

**Expectations:** no parsing errors, no ommited small clusters

**Call:**

```
time ./pooling.py context-eval/data/Inventory-TWSI-2.csv 2333 model/wiki-sz300-w3-cb1-it3-min20.w2v model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.mean -inventory intermediate/twsi-clusters_inventory.csv --no_header
```

**Output:**

```
Sense vectors: 2333, duplicates: 0, small: 0, clusters: 2333
Saving sense vectors...

real    2m23.212s
user    2m17.288s
sys     0m5.188s
```

**Observations:**
Expectations are met.

### Pooling of sense vectors (any cluster size, weighted method)

**Expectations:** nothing new. Identical inventory.

**Call:**

```
script intermediate/twsi_poolweighted.log

time ./pooling.py context-eval/data/Inventory-TWSI-2.csv 2333 model/wiki-sz300-w3-cb1-it3-min20.w2v model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted --no_header -method weighted_mean
```

**Output:**

```
Loading original word model...
Vector size = 300
Initializing Word2Vec object for sense vectors...
Pooling cluster vectors (weighted_mean method)...
Progress: 0%  Column types: word       object
cid         int64
cluster    object
dtype: object                                                                                                                               100%
Sense vectors: 2333, duplicates: 0, small: 0, clusters: 2333
Saving sense vectors...

real    2m37.956s
user    2m30.204s
sys     0m5.804s
```

**Observations:**
Identical inventories, delete intermediate/twsi-clusters_inventory_weighted.csv. Assert if vectors differ for mean method and weighted.

### Upper bound evaluation
**Expectations:** 1 and 1 for precision and recall, full coverage.

**Call:**

```
time python twsi_upper_bound.py ~/experiment/intermediate/twsi-clusters_inventory.csv -predictions ~/experiment/context-eval/data/TWSI_dev.csv
```

**Output:**

```
Warning: wrong cluster word rel_terms
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: tv
Warning: skipping word not present in TWSI vocabulary: tv
Mapping: data/Mapping_Inventory-TWSI-2.csv_twsi-clusters_inventory.csv
twsi #words: 1010
twsi #senses: 2285.0
twsi average #senses per word: 2.26
user #words: 1010
user #senses: 2330.0
user average #senses per word: 2.31
 user unmapped senses: 0.30% (7 of 1010)
user unmapped words: []
twsi unmapped senses:
 twsi unmapped senses: 0.00% (0 of 2285)
Estimating upper bound performance:  /home/pelevina/experiment/context-eval/data/TWSI_dev.csv

Upper Bound Results:
Correct, retrieved, nr_sentences
6154    6165
Precision: 1.0  Recall: 0.998215733982  F1: 0.999107070379

real    0m23.970s
user    0m22.132s
sys     0m1.760s
```

**Observations:**
11 "mistakes" are made, how can it happen? Identify problematic rows:

connection 6306564
TV 10812559
TV 12517568
TV 12884703
TV 14612113
TV 14944277
TV 11121473
TV 1225385
TV 1238069
TV 13039441
TV 15537327

There are two test instances for connection+6306564 labeled in two different senses (1 and 4 twsi senses). Therefore, the second is skipped when ```correct``` is incremented. 

TV problem is caused by something being lowercased (target word of user inventory in mapping function).

### Prediction

**Expecation:**
none

**Call:**

```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.mean model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.mean.csv

time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.csv
```

**Output:**

```
â€¦


Loading models...
Loading test set...
6165 test instances
Start prediction over context-eval/data/TWSI_dev.csv
Progress: 100%
Saved predictions to eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.csv

real    3m1.938s
user    2m55.368s
sys     0m5.572s
```

### Evaluation
**Expecation:**
Lower value of precision signals faults of the wsd approach. What does low recall mean? In our case probably the same (cause recall is different from precision only if we decide not to predict something). Hopefully mean results will be different from results of weighted.

**Call:**

```
time python twsi_evaluation.py ~/experiment/intermediate/twsi-clusters_inventory.csv ~/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.mean.csv

time python twsi_evaluation.py ~/experiment/intermediate/twsi-clusters_inventory.csv ~/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.csv
```

**Output:**

```
Loading assigned TWSI senses...
Loading TWSI sense inventory...
Loading provided Sense Inventory /home/pelevina/experiment/intermediate/twsi-clusters_inventory.csv...
Warning: wrong cluster word rel_terms
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: tv
Warning: skipping word not present in TWSI vocabulary: tv
Mapping: data/Mapping_Inventory-TWSI-2.csv_twsi-clusters_inventory.csv
twsi #words: 1010
twsi #senses: 2285.0
twsi average #senses per word: 2.26
user #words: 1010
user #senses: 2330.0
user average #senses per word: 2.31
 user unmapped senses: 0.30% (7 of 1010)
user unmapped words: []
twsi unmapped senses:
 twsi unmapped senses: 0.00% (0 of 2285)
Evaluating Predicted Labels /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.mean.csv...
Evaluated dataset: /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.mean.csv-evaluated.csv

Evaluation Results:
Correct, retrieved, nr_sentences
2979    6154    6165
Precision: 0.484075398115       Recall: 0.483211678832  F1: 0.483643152853
Coverage:  0.998215733982

real    0m26.049s
user    0m23.820s
sys     0m1.872s


Loading assigned TWSI senses...
Loading TWSI sense inventory...
Loading provided Sense Inventory /home/pelevina/experiment/intermediate/twsi-clusters_inventory.csv...
Warning: wrong cluster word rel_terms
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: cd
Warning: skipping word not present in TWSI vocabulary: tv
Warning: skipping word not present in TWSI vocabulary: tv
Mapping: data/Mapping_Inventory-TWSI-2.csv_twsi-clusters_inventory.csv
twsi #words: 1010
twsi #senses: 2285.0
twsi average #senses per word: 2.26
user #words: 1010
user #senses: 2330.0
user average #senses per word: 2.31
 user unmapped senses: 0.30% (7 of 1010)
user unmapped words: []
twsi unmapped senses:
 twsi unmapped senses: 0.00% (0 of 2285)
Evaluating Predicted Labels /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.csv...
Evaluated dataset: /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.csv-evaluated.csv

Evaluation Results:
Correct, retrieved, nr_sentences
3211    6154    6165
Precision: 0.521774455639       Recall: 0.520843471208  F1: 0.521308547772
Coverage:  0.998215733982

real    0m26.973s
user    0m25.216s
sys     0m1.704s

```

**Observations** 
Quite poor results. It means more than half of senses is predicted incorrectly only because of bad vector pooling/prediction approach? Expect errors. Well, it does indeed make a lot of prediction mistakes. Partially it is already noticeable that sense vector may produce neighbours which are quite different from the cluser words they were based on. Plus the wsd on any particular context mey be not perfect.

weighted is indeed a little bit better!

### Try different disambiguation method

**Goal:** so far we disambiguate by calculating probabilities of a sense  with each word in context. Try a different approach: first pool all context vectors, then calculate probabilities of senses given one context vector. Adapted wsd.py. Repeat prediction and evaluation for weighted sense vectors. Compare results.

### Check if wsd.py changes was correct (repeat weighted test with basic method)

**Expectations:** identical results as in previous experiment, no errors. Perform prediction and evaluation together

**Call:**

```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted2.csv -wsd_method sep

time python twsi_evaluation.py ~/experiment/intermediate/twsi-clusters_inventory.csv ~/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted2.csv
```

**Output:**

```
Evaluated dataset: /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted2.csv-evaluated.csv

Evaluation Results:
Correct, retrieved, nr_sentences
3211 	6154 	6165
Precision: 0.521774455639 	Recall: 0.520843471208 	F1: 0.521308547772
Coverage:  0.998215733982

real	0m19.533s
user	0m15.848s
sys	0m1.552s
```

**Observations:**
Everything fine, identical results. Delete prediction and evaluation files.

### Test new wsd approach

**Call:**

```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv -wsd_method avg && cd context-eval && time python twsi_evaluation.py ~/experiment/intermediate/twsi-clusters_inventory.csv ~/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv
```

**Output:**
```
Saved predictions to eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv

real	1m47.952s
user	1m43.592s
sys	0m4.056s

Sense inventory: /home/pelevina/experiment/intermediate/twsi-clusters_inventory.csv

Evaluating Predicted Labels /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv...
Evaluated dataset: /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv-evaluated.csv

Evaluation Results:
Correct, retrieved, nr_sentences
3213 	6154 	6165
Precision: 0.522099447514 	Recall: 0.521167883212 	F1: 0.521633249452
Coverage:  0.998215733982

real	0m17.094s
user	0m15.892s
sys	0m1.192s
```

**Observations:**
Gained only 2 new correct predictions :( Compare with the gain on wiki dataset 

### Try third method

**Description**
Now use *cosine similarity* between an average context vector and a sense vector.

**Call:**

```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.sim.csv -wsd_method sim && cd context-eval && time python twsi_evaluation.py ~/experiment/intermediate/twsi-clusters_inventory.csv ~/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.sim.csv
```

**Output:**

```
Evaluating Predicted Labels /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.sim.csv...
Evaluated dataset: /home/pelevina/experiment/eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.sim.csv-evaluated.csv

Evaluation Results:
Correct, retrieved, nr_sentences
3213 	6154 	6165
Precision: 0.522099447514 	Recall: 0.521167883212 	F1: 0.521633249452
Coverage:  0.998215733982
```

**Observation:** same results?!


###  Analyze confidence values

**Goal:** Define confidence as P(max_sense)/sum_i(P(sense_i)). Observe values, especially corelation between confidence value and total number of senses per word. Use weighted pooling approach and averaged context.

**Expectation:** Confidence values for predictions with more possible senses are expected to be lower.

**Call:**

```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.avg.csv -wsd_method avg
```

**Observation:**
Output in ```eval/test_conf_avg.txt```
* There is no correlation between confidence value of prediction and its correctness. Often, wrong predictions are more confident, than the right ones.
* Confidence values indeed depend on number of possible senses. For two senses they are around 0.5, for three senses around 0.33 etc.

###  Analyze confidence values 2
**Goal:** repeat on prediction with weighted sense vectors and sep disambiguation method

**Call:**

```
Output in ```eval/test_conf_sep.txt```
time ./prediction.py context-eval/data/TWSI_dev.csv model/wiki-sz300-w3-cb1-it3-min20.w2v.senses.twsi.weighted model/wiki-sz300-w3-cb1-it3-min20.w2v.contexts eval/TWSI_dev_wiki-sz300-w3-cb1-it3-min20.twsi.weighted.sep.csv -wsd_method sep
```

**Observation:**
Output in ```eval/test_conf_sep.txt```

* Also no correlation observed
* Again, confidence values depend on the number of senses, diversity of values is a little bit higher than for predictions with avg method

 