### 9 March: Collect 200 nearest neighbours for wiki word vectors
**Expectations:** none

**Call:** 

```
time python word_neighbours.py model/wiki-sz300-w3-cb1-it3-min20.w2v intermediate/wiki_neighbours_1mil.csv -vocab_limit 1000000
```

**Output:**

```
Saving word neighbours to intermediate/wiki_neighbours_1mil.csv
Progress: 100%
Number of processed words: 1000000

real    19513m47.976s
user    19436m24.256s
sys     11m57.576s
```

**Observations:** 

* 1 week -> 47% of the first 1 milion words
* 47% -> ~2.2GB neighbours file 
* Set threshold for proximity and ignore neighbours that are too far away to decrease file size.
* Finished in the end

### 15 March: Neighbours analysis for first 300000 words in vocab
**Expectation:** 

* Neighbours for dictionary words (real english words) would also be words. Neighbours for garbage (12:20, http etc.) will also be garbage
* Some proximity threshold may be found that should enable to cut off unimportant neighbours to reduce file size.

**Call:** same as above but on the university cluster. Interrupted after 11 hours (run limit)

**Observations:**

* ~ 11 hours (run limit) to collect 300000 words using 16 nodes
* Many garbage words (numbers, dates) appear even in dictionary words. For example 'weekend' has [Saturday, month, day, holiday] among first 50 neighbours and [2014, 10:00pm, 8:00pm etc] among last neighbours. Such dates/times amount to at least 30-40% of all 'weekend' neighbours. Their proximity is between 0.48 - 0.39.
* All proximity values are above 0.3 which is to large to use as a cut-off threshold (from my experience important neighbours might be further from ego-centre). Maybe outliers - words that are far (~0.09) from all its neighbours - will be found amoung next 300000 vocabulary words.

**Idea:** fall back to word2vec default handling of numbers in the corpus: replace any digit with it written form and wrap it in whitespaces. Pros: decrease the model size and, quite importnatly, DT files with neighbours for CW input. Also it would help to avoid cluster parsing errors (at words like "18:00" or "0,0374"). Cons: disrupts syntactical structure of corpus: "planted 23000 apple trees" -> "planted two three zero zero zero apple trees"

### 30 March: Neighbours analysis for 1mil words in vocab

* 200 neighbours, 5GB, too big for clustering algorithm
* remove any pair with similarity lower than 0.3

``` 
python prune_neighbours.py wiki_neighbours_1mil.csv wiki_neighbours_1mil_pruned.csv
```
* decreased size only by 90 MB :()
* discard the idea, just run CW for first 550k words (550k*200 = 110 000 000 lines)

```
head -110000000 intermediate/wiki_neighbours_1mil.csv > intermediate/wiki_neighbours_550k.csv
```

### 30 March: CW clustering for 550k neighbours
```
time java -Xms2G -Xmx100G -cp chinese-whispers/target/chinese-whispers.jar de.tudarmstadt.lt.wsi.WSI -in intermediate/wiki_neighbours_550k.csv -n 200 -N 200 -out intermediate/wiki_neighbours_550k_clusters.csv -clustering cw
```

**Output:**

```
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fe78cf00000, 5224529920, 0) failed; error='Cannot allocate memory' (errno=12)
#
 There is insufficient memory for the Java Runtime Environment to continue.
 Native memory allocation (malloc) failed to allocate 5224529920 bytes for committing reserved memory.
 An error report file with more information is saved as:
 /srv/home/pelevina/experiment/hs_err_pid3292.log

real    30m35.267s
user    70m58.620s
sys     1m6.064s
```


### 19 March: Collect 200 neighbours for wiki word vectors (fast computation)
**Expectations:** Collect neighbours and filter any token that contains anything other than ascii letters, dot and hyphen. Should drastically decrease the size (from 5GB).

**Call:** 

```
time python word2vec_utils/similar_top.py model/wiki-sz300-w3-cb1-it3-min20.w2v -output intermediate/test.gz -pairs -vocab_limit 15 -only_letters

time python word2vec_utils/similar_top.py model/wiki-sz300-w3-cb1-it3-min20.w2v -output intermediate/wiki_neighbours_1mil_letters.gz -pairs -vocab_limit 1000000 -only_letters
```

**Output:**

### Count frequencies in wiki corpora
**Goal:** use frequencies for neighbours collection (extract not only close, but close and frequent neighbours)
**Expecataion:** same ordering as in the model (compare to neighbours file)

**Call:**

```
time python frequencies.py wiki.txt wiki_freq_min20.txt
```

**Output:**

```
real	41m1.745s
user	40m30.844s
sys	0m16.608s
```

**Observation:** according to first 10 words the ordering is correct.

