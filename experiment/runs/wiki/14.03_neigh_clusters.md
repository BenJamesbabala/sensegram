### 9 March: Collect 200 nearest neighbours for wiki word vectors
**Expectations:** none

**Call:** 

```
time python word_neighbours.py model/wiki-sz300-w3-cb1-it3-min20.w2v intermediate/wiki_neighbours_1mil.csv -vocab_limit 1000000
```

**Observations:** 

* 1 week -> 47% of the first 1 milion words
* 47% -> ~2.2GB neighbours file 
* Set threshold for proximity and ignore neighbours that are too far away to decrease file size.

### 15 March: Neighbours analysis for first 300000 words in vocab
**Expectation:** 

* Neighbours for dictionary words (real english words) would also be words. Neighbours for garbage (12:20, http etc.) will also be garbage
* Some proximity threshold may be found that should enable to cut off unimportant neighbours to reduce file size.

**Call:** same as above but on the university cluster. Interrupted after 11 hours (run limit)

**Observations:**

* ~ 11 hours (run limit) to collect 300000 words using 16 nodes
* Many garbage words (numbers, dates) appear even in dictionary words. For example 'weekend' has [Saturday, month, day, holiday] among first 50 neighbours and [2014, 10:00pm, 8:00pm etc] among last neighbours. Such dates/times amount to at least 30-40% of all 'weekend' neighbours. Their proximity is between 0.48 - 0.39.
* All proximity values are above 0.3 which is to large to use as a cut-off threshold (from my experience important neighbours might be further from ego-centre). Maybe outliers - words that are far (~0.09) from all its neighbours - will be found amoung next 300000 vocabulary words.

**Idea:** fall back to word2vec default handling of numbers in the corpus: replace any digit with it written form and wrap it in whitespaces. Pros: decrease the model size and, quite importnatly, DT files with neighbours for CW input. Also it would help to avoid cluster parsing errors (at words like "18:00" or "0,0374"). Cons: disrupts syntactical structure of corpus: "planted 23000 apple trees" -> "planted two three zero zero zero apple trees"
